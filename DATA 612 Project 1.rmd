---
title: "Global Baseline Predictors and RMSE "
subtitle: "Data 612 Project 1"
author: "William Outcault"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document:
    toc: yes
  html_document:
    highlight: pygments
    theme: cerulean
    toc: yes
    toc_float:
      collapsed: yes
---

\newpage

# Composing the Dataset

The following dataset is made up of 6 rows and 5 columns, the data was fabricated by myself. In context this data will represent movies rated by users A-F (names will be private), and movies will be labelled 1-5 (for sake of convenience I assigned the movies to numbers). This system will recommend movies to its users.

```{r message = F, warning = F}
m <- matrix(
  c(5,4,4,2,4,4,NA,3,2,2,NA,2,4,5,NA,3,5,5,NA,3,NA,1,4,4,4,4,3,NA,5,4,3,2,1,NA,4,3),
  nrow = 6,
  dimnames = list(c("A","B","C","D","E","F"), c("1","2","3","4","5","6")))

knitr::kable(m)
```

# Splitting the Testing/Training Sets 

Next we will create our training and testing sets by random sampling. We will use 75% of the data for the training and 25% for the testing.

```{r}
smp_size <- floor(0.75 * nrow(m))

set.seed(123)
train_ind <- sample(seq_len(nrow(m)), size = smp_size)

train <- m[train_ind, ]
test <- m[-train_ind, ]
```

```{r echo=F}
knitr::kable(train)

knitr::kable(test)
```


\newpage

# Raw Average and RMSE

Now that we have our training/testing sets we will make our first predictions. We will begin by finding the mean for all values, replacing our unknown values with the mean and make our prediction using our mean values.

```{r}
avg <- mean(train[!is.na(train)])
rmse_train <- round(mean((train[!is.na(train)]-avg)^2),3)
rmse_test <- round(mean((test[!is.na(test)]-avg)^2),3)
```

The raw average rating for every user-item combination is `r avg`. The RMSE for both our training data and testing data is `r rmse_train` and `r rmse_test` respectively.

\newpage

# Training

We begin by finding user-item bias. This is done by taking the average for each row (user) and column (item).

## Bias

```{r}
user_bias <- c()
item_bias <- c()

for (i in seq(1:nrow(train))){
  user_avg <- mean(train[i,!is.na(train[i,])])
  user_bias <- append(user_bias,user_avg-avg)
}

for (i in seq(1:ncol(train))){
  item_avg <- mean(train[!is.na(train[,i]),i])
  item_bias <- append(item_bias,item_avg-avg)
}
```

```{r echo = F}
knitr::kable(user_bias, col.names = "User Bias")
```

```{r echo = F}
knitr::kable(item_bias, col.names=c("Item Bias"))
```

\newpage

## Baseline Predictors

Our baseline predictors are found by adding each user-item bias in addition to raw average.

```{r}
baseline_train_matrix <- matrix(
  c(5,4,4,2,4,4,NA,3,2,2,NA,2,4,5,NA,3,5,5,NA,3,NA,1,4,4),
  nrow = 4,
  dimnames = list(c("C","F","B","D"), c("1","2","3","4","5","6")))

for (i in seq(1:nrow(train))){
  for (j in seq(1:ncol(train))){
    baseline_train_matrix[i,j] <- round(avg+user_bias[i]+item_bias[j],2)  
    if (baseline_train_matrix[i,j] > 5){
      baseline_train_matrix[i,j] <- 5
    }
    if (baseline_train_matrix[i,j] < 0){
      baseline_train_matrix[i,j] <- 0
    }
  }
}

knitr::kable(baseline_train_matrix)
```

## RMSE for Baseline Predictors

We use our baseline predictions to find our route mean square error.

```{r}
which_na <- train[!is.na(train)]
rmse_train <- round(mean((train[which_na]-baseline_train_matrix[which_na])^2),3)
```

The RMSE for the training set using baseline predictors is `r rmse_train`.

\newpage

# Testing Set 

We repeat this process except we are using our test set.

## Bias

```{r}
user_bias <- c()
item_bias <- c()

for (i in seq(1:nrow(test))){
  user_avg <- mean(test[i,!is.na(test[i,])])
  user_bias <- append(user_bias,user_avg-avg)
}

for (i in seq(1:ncol(test))){
  item_avg <- mean(test[!is.na(test[,i]),i])
  item_bias <- append(item_bias,item_avg-avg)
}
```

```{r echo = F}
knitr::kable(user_bias, col.names = "User Bias")
```

```{r echo = F}
knitr::kable(item_bias, col.names=c("Item Bias"))
```

\newpage

## Baseline Predictors

```{r}
baseline_test_matrix <- matrix(
  c(5,4,4,2,4,4,NA,3,2,2,NA,2),
  nrow = 2,
  dimnames = list(c("A","E"), c("1","2","3","4","5","6")))

item_bias[is.na(item_bias)] <- 0

for (i in seq(1:nrow(test))){
  for (j in seq(1:ncol(test))){
    baseline_test_matrix[i,j] <- round(avg+user_bias[i]+item_bias[j],2)  
    if (baseline_test_matrix[i,j] > 5){
      baseline_test_matrix[i,j] <- 5
    }
    if (baseline_test_matrix[i,j] < 0){
      baseline_test_matrix[i,j] <- 0
    }
  }
}

knitr::kable(baseline_test_matrix)
```

## RMSE for Baseline Predictors

We use our baseline predictions to find our route mean square error.

```{r}
which_na <- !is.na(test)
rmse_test <- round(mean((test[which_na]-baseline_test_matrix[which_na])^2),3)
```

The RMSE for the training set using baseline predictors is `r rmse_test`.

# Conclusion

Our RMSE for our training and test set using baseline predictions are `r rmse_train` and `r rmse_test` respectively. Our test RSME is signifcantly larger than the training, intuitively I would believe this is because our test set is smaller than our training set. This would lead to a higher variance therefore a chance for a much higher RMSE. If given larger datasets we would find that our baseline predictions produce similar RMSE scores for both our training and test set.


